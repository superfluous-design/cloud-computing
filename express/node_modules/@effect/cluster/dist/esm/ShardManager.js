/**
 * @since 1.0.0
 */
import * as Rpc from "@effect/rpc/Rpc";
import * as RpcClient from "@effect/rpc/RpcClient";
import * as RpcGroup from "@effect/rpc/RpcGroup";
import * as RpcServer from "@effect/rpc/RpcServer";
import * as Arr from "effect/Array";
import * as Clock from "effect/Clock";
import * as Config_ from "effect/Config";
import * as ConfigProvider from "effect/ConfigProvider";
import * as Context from "effect/Context";
import * as Data from "effect/Data";
import * as Deferred from "effect/Deferred";
import * as Duration from "effect/Duration";
import * as Effect from "effect/Effect";
import * as Equal from "effect/Equal";
import * as FiberSet from "effect/FiberSet";
import { identity } from "effect/Function";
import * as Iterable from "effect/Iterable";
import * as Layer from "effect/Layer";
import * as Mailbox from "effect/Mailbox";
import * as Metric from "effect/Metric";
import * as MutableHashMap from "effect/MutableHashMap";
import * as MutableHashSet from "effect/MutableHashSet";
import * as Option from "effect/Option";
import * as PubSub from "effect/PubSub";
import * as Queue from "effect/Queue";
import * as Schedule from "effect/Schedule";
import * as Schema from "effect/Schema";
import { RunnerNotRegistered } from "./ClusterError.js";
import * as ClusterMetrics from "./ClusterMetrics.js";
import { decideAssignmentsForUnassignedShards, decideAssignmentsForUnbalancedShards, RunnerWithMetadata, State } from "./internal/shardManager.js";
import * as MachineId from "./MachineId.js";
import { Runner } from "./Runner.js";
import { RunnerAddress } from "./RunnerAddress.js";
import { RunnerHealth } from "./RunnerHealth.js";
import { RpcClientProtocol, Runners } from "./Runners.js";
import { ShardId } from "./ShardId.js";
import { ShardingConfig } from "./ShardingConfig.js";
import { ShardStorage } from "./ShardStorage.js";
/**
 * @since 1.0.0
 * @category models
 */
export class ShardManager extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager")() {}
/**
 * @since 1.0.0
 * @category Config
 */
export class Config extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/Config")() {
  /**
   * @since 1.0.0
   */
  static defaults = {
    rebalanceDebounce: /*#__PURE__*/Duration.millis(500),
    rebalanceInterval: /*#__PURE__*/Duration.seconds(20),
    rebalanceRetryInterval: /*#__PURE__*/Duration.seconds(10),
    rebalanceRate: 2 / 100,
    persistRetryCount: 100,
    persistRetryInterval: /*#__PURE__*/Duration.seconds(3),
    runnerHealthCheckInterval: /*#__PURE__*/Duration.minutes(1),
    runnerPingTimeout: /*#__PURE__*/Duration.seconds(3)
  };
}
/**
 * @since 1.0.0
 * @category Config
 */
export const configConfig = /*#__PURE__*/Config_.all({
  rebalanceDebounce: /*#__PURE__*/Config_.duration("rebalanceDebounce").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceDebounce), /*#__PURE__*/Config_.withDescription("The duration to wait before rebalancing shards after a change.")),
  rebalanceInterval: /*#__PURE__*/Config_.duration("rebalanceInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceInterval), /*#__PURE__*/Config_.withDescription("The interval on which regular rebalancing of shards will occur.")),
  rebalanceRetryInterval: /*#__PURE__*/Config_.duration("rebalanceRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which rebalancing of shards which failed to be rebalanced will be retried.")),
  rebalanceRate: /*#__PURE__*/Config_.number("rebalanceRate").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRate), /*#__PURE__*/Config_.withDescription("The maximum ratio of shards to rebalance at once.")),
  persistRetryCount: /*#__PURE__*/Config_.integer("persistRetryCount").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryCount), /*#__PURE__*/Config_.withDescription("The number of times persistence of runners will be retried if it fails.")),
  persistRetryInterval: /*#__PURE__*/Config_.duration("persistRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which persistence of runners will be retried if it fails.")),
  runnerHealthCheckInterval: /*#__PURE__*/Config_.duration("runnerHealthCheckInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerHealthCheckInterval), /*#__PURE__*/Config_.withDescription("The interval on which runner health will be checked.")),
  runnerPingTimeout: /*#__PURE__*/Config_.duration("runnerPingTimeout").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerPingTimeout), /*#__PURE__*/Config_.withDescription("The length of time to wait for a runner to respond to a ping."))
});
/**
 * @since 1.0.0
 * @category Config
 */
export const configFromEnv = /*#__PURE__*/configConfig.pipe(/*#__PURE__*/Effect.withConfigProvider(/*#__PURE__*/ConfigProvider.fromEnv().pipe(ConfigProvider.constantCase)));
/**
 * @since 1.0.0
 * @category Config
 */
export const layerConfig = config => Layer.succeed(Config, {
  ...Config.defaults,
  ...config
});
/**
 * @since 1.0.0
 * @category Config
 */
export const layerConfigFromEnv = /*#__PURE__*/Layer.effect(Config, configFromEnv);
/**
 * Represents a client which can be used to communicate with the
 * `ShardManager`.
 *
 * @since 1.0.0
 * @category Client
 */
export class ShardManagerClient extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/ShardManagerClient")() {}
/**
 * @since 1.0.0
 * @category models
 */
export const ShardingEventSchema = /*#__PURE__*/Schema.Union(/*#__PURE__*/Schema.TaggedStruct("StreamStarted", {}), /*#__PURE__*/Schema.TaggedStruct("ShardsAssigned", {
  address: RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("ShardsUnassigned", {
  address: RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("RunnerRegistered", {
  address: RunnerAddress
}), /*#__PURE__*/Schema.TaggedStruct("RunnerUnregistered", {
  address: RunnerAddress
}));
/**
 * The messaging protocol for the `ShardManager`.
 *
 * @since 1.0.0
 * @category Rpcs
 */
export class Rpcs extends /*#__PURE__*/RpcGroup.make(/*#__PURE__*/Rpc.make("Register", {
  payload: {
    runner: Runner
  },
  success: MachineId.MachineId
}), /*#__PURE__*/Rpc.make("Unregister", {
  payload: {
    address: RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("NotifyUnhealthyRunner", {
  payload: {
    address: RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("GetAssignments", {
  success: /*#__PURE__*/Schema.ReadonlyMap({
    key: ShardId,
    value: /*#__PURE__*/Schema.Option(RunnerAddress)
  })
}), /*#__PURE__*/Rpc.make("ShardingEvents", {
  success: ShardingEventSchema,
  stream: true
}), /*#__PURE__*/Rpc.make("GetTime", {
  success: Schema.Number
})) {}
/**
 * @since 1.0.0
 * @category models
 */
export const ShardingEvent = /*#__PURE__*/Data.taggedEnum();
/**
 * @since 1.0.0
 * @category Client
 */
export const makeClientLocal = /*#__PURE__*/Effect.gen(function* () {
  const runnerAddress = yield* ShardingConfig;
  const clock = yield* Effect.clock;
  const shards = new Map();
  for (let n = 1; n <= runnerAddress.numberOfShards; n++) {
    shards.set(ShardId.make(n), runnerAddress.runnerAddress);
  }
  let machineId = 0;
  return ShardManagerClient.of({
    register: () => Effect.sync(() => MachineId.make(++machineId)),
    unregister: () => Effect.void,
    notifyUnhealthyRunner: () => Effect.void,
    getAssignments: Effect.succeed(shards),
    shardingEvents: Effect.gen(function* () {
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      return mailbox;
    }),
    getTime: clock.currentTimeMillis
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
export const makeClientRpc = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const client = yield* RpcClient.make(Rpcs, {
    spanPrefix: "ShardManagerClient",
    disableTracing: true
  });
  return ShardManagerClient.of({
    register: address => client.Register({
      runner: Runner.make({
        address,
        version: config.serverVersion
      })
    }),
    unregister: address => client.Unregister({
      address
    }),
    notifyUnhealthyRunner: address => client.NotifyUnhealthyRunner({
      address
    }),
    getAssignments: client.GetAssignments(),
    shardingEvents: client.ShardingEvents({}, {
      asMailbox: true
    }),
    getTime: client.GetTime()
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
export const layerClientLocal = /*#__PURE__*/Layer.effect(ShardManagerClient, makeClientLocal);
/**
 * @since 1.0.0
 * @category Client
 */
export const layerClientRpc = /*#__PURE__*/Layer.scoped(ShardManagerClient, makeClientRpc).pipe(/*#__PURE__*/Layer.provide(/*#__PURE__*/Layer.scoped(RpcClient.Protocol, /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const clientProtocol = yield* RpcClientProtocol;
  return yield* clientProtocol(config.shardManagerAddress);
}))));
/**
 * @since 1.0.0
 * @category Constructors
 */
export const make = /*#__PURE__*/Effect.gen(function* () {
  const storage = yield* ShardStorage;
  const runnersApi = yield* Runners;
  const runnerHealthApi = yield* RunnerHealth;
  const clock = yield* Effect.clock;
  const config = yield* Config;
  const shardingConfig = yield* ShardingConfig;
  const state = yield* Effect.orDie(State.fromStorage(shardingConfig.numberOfShards));
  const scope = yield* Effect.scope;
  const events = yield* PubSub.unbounded();
  yield* Metric.incrementBy(ClusterMetrics.runners, MutableHashMap.size(state.runners));
  for (const address of state.shards.values()) {
    const metric = Option.isSome(address) ? Metric.tagged(ClusterMetrics.assignedShards, "address", address.toString()) : ClusterMetrics.unassignedShards;
    yield* Metric.increment(metric);
  }
  function withRetry(effect) {
    return effect.pipe(Effect.retry({
      schedule: Schedule.spaced(config.persistRetryCount),
      times: config.persistRetryCount
    }), Effect.ignore);
  }
  const persistRunners = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveRunners(Iterable.map(state.runners, ([address, runner]) => [address, runner.runner])))));
  const persistAssignments = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveAssignments(state.shards))));
  const notifyUnhealthyRunner = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.runners, address)) return;
    yield* Metric.increment(Metric.tagged(ClusterMetrics.runnerHealthChecked, "runner_address", address.toString()));
    if (!(yield* runnerHealthApi.isAlive(address))) {
      yield* Effect.logWarning(`Runner at address '${address.toString()}' is not alive`);
      yield* unregister(address);
    }
  });
  function updateShardsState(shards, address) {
    return Effect.suspend(() => {
      if (Option.isSome(address) && !MutableHashMap.has(state.runners, address.value)) {
        return Effect.fail(new RunnerNotRegistered({
          address: address.value
        }));
      }
      for (const shardId of shards) {
        if (!state.shards.has(shardId)) continue;
        state.shards.set(shardId, address);
      }
      return Effect.void;
    });
  }
  const getAssignments = Effect.sync(() => state.shards);
  let machineId = 0;
  const register = Effect.fnUntraced(function* (runner) {
    yield* Effect.logInfo(`Registering runner ${Runner.pretty(runner)}`);
    const now = clock.unsafeCurrentTimeMillis();
    MutableHashMap.set(state.runners, runner.address, RunnerWithMetadata({
      runner,
      registeredAt: now
    }));
    yield* Metric.increment(ClusterMetrics.runners);
    yield* PubSub.publish(events, ShardingEvent.RunnerRegistered({
      address: runner.address
    }));
    if (state.unassignedShards.length > 0) {
      yield* rebalance(false);
    }
    yield* Effect.forkIn(persistRunners, scope);
    return MachineId.make(++machineId);
  });
  const unregister = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.runners, address)) return;
    yield* Effect.logInfo("Unregistering runner at address:", address);
    const unassignments = Arr.empty();
    for (const [shard, runner] of state.shards) {
      if (Option.isSome(runner) && Equal.equals(runner.value, address)) {
        unassignments.push(shard);
        state.shards.set(shard, Option.none());
      }
    }
    MutableHashMap.remove(state.runners, address);
    yield* Metric.incrementBy(ClusterMetrics.runners, -1);
    if (unassignments.length > 0) {
      yield* Metric.incrementBy(Metric.tagged(ClusterMetrics.unassignedShards, "runner_address", address.toString()), unassignments.length);
      yield* PubSub.publish(events, ShardingEvent.RunnerUnregistered({
        address
      }));
    }
    yield* Effect.forkIn(persistRunners, scope);
    yield* Effect.forkIn(rebalance(true), scope);
  });
  let rebalancing = false;
  let nextRebalanceImmediate = false;
  let rebalanceDeferred;
  const rebalanceFibers = yield* FiberSet.make();
  const rebalance = immmediate => Effect.withFiberRuntime(fiber => {
    if (!rebalancing) {
      rebalancing = true;
      return rebalanceLoop(immmediate);
    }
    if (immmediate) {
      nextRebalanceImmediate = true;
    }
    if (!rebalanceDeferred) {
      rebalanceDeferred = Deferred.unsafeMake(fiber.id());
    }
    return Deferred.await(rebalanceDeferred);
  });
  const rebalanceLoop = immediate => Effect.suspend(() => {
    const deferred = rebalanceDeferred;
    rebalanceDeferred = undefined;
    if (!immediate) {
      immediate = nextRebalanceImmediate;
      nextRebalanceImmediate = false;
    }
    return runRebalance(immediate).pipe(deferred ? Effect.intoDeferred(deferred) : identity, Effect.onExit(() => {
      if (!rebalanceDeferred) {
        rebalancing = false;
        return Effect.void;
      }
      return Effect.forkIn(rebalanceLoop(), scope);
    }));
  });
  const runRebalance = Effect.fn("ShardManager.rebalance")(function* (immediate) {
    yield* Effect.annotateCurrentSpan("immmediate", immediate);
    yield* Effect.sleep(config.rebalanceDebounce);
    // Determine which shards to assign and unassign
    const [assignments, unassignments, changes] = immediate || state.unassignedShards.length > 0 ? decideAssignmentsForUnassignedShards(state) : decideAssignmentsForUnbalancedShards(state, config.rebalanceRate);
    yield* Effect.logDebug(`Rebalancing shards (immediate = ${immediate})`);
    if (MutableHashSet.size(changes) === 0) return;
    yield* Metric.increment(ClusterMetrics.rebalances);
    // Ping runners first and remove unhealthy ones
    const failedRunners = MutableHashSet.empty();
    for (const address of changes) {
      yield* FiberSet.run(rebalanceFibers, runnersApi.ping(address).pipe(Effect.timeout(config.runnerPingTimeout), Effect.catchAll(() => {
        MutableHashSet.add(failedRunners, address);
        MutableHashMap.remove(assignments, address);
        MutableHashMap.remove(unassignments, address);
        return Effect.void;
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    const failedUnassignments = new Set();
    for (const [address, shards] of unassignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.none()).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          for (const shard of shards) {
            failedUnassignments.add(shard);
          }
          // Remove failed runners from the assignments
          MutableHashMap.remove(assignments, address);
          return Effect.void;
        },
        onSuccess: () => {
          const shardCount = shards.size;
          return Metric.incrementBy(Metric.tagged(ClusterMetrics.assignedShards, "runner_address", address.toString()), -shardCount).pipe(Effect.zipRight(Metric.incrementBy(ClusterMetrics.unassignedShards, shardCount)), Effect.zipRight(PubSub.publish(events, ShardingEvent.ShardsUnassigned({
            address,
            shards: Array.from(shards)
          }))));
        }
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    // Remove failed shard unassignments from the assignments
    MutableHashMap.forEach(assignments, (shards, address) => {
      for (const shard of failedUnassignments) {
        shards.delete(shard);
      }
      if (shards.size === 0) {
        MutableHashMap.remove(assignments, address);
      }
    });
    // Perform the assignments
    for (const [address, shards] of assignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.some(address)).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          return Effect.void;
        },
        onSuccess: () => {
          const shardCount = shards.size;
          return Metric.incrementBy(Metric.tagged(ClusterMetrics.assignedShards, "runner_address", address.toString()), -shardCount).pipe(Effect.zipRight(Metric.incrementBy(ClusterMetrics.unassignedShards, -shardCount)), Effect.zipRight(PubSub.publish(events, ShardingEvent.ShardsAssigned({
            address,
            shards: Array.from(shards)
          }))));
        }
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    const wereFailures = MutableHashSet.size(failedRunners) > 0;
    if (wereFailures) {
      // Check if the failing runners are still reachable
      yield* Effect.forEach(failedRunners, notifyUnhealthyRunner, {
        discard: true
      }).pipe(Effect.forkIn(scope));
      yield* Effect.logWarning("Failed to rebalance runners: ", failedRunners);
    }
    if (wereFailures && immediate) {
      // Try rebalancing again later if there were any failures
      yield* Clock.sleep(config.rebalanceRetryInterval).pipe(Effect.zipRight(rebalance(immediate)), Effect.forkIn(scope));
    }
    yield* persistAssignments;
  });
  const checkRunnerHealth = Effect.suspend(() => Effect.forEach(MutableHashMap.keys(state.runners), notifyUnhealthyRunner, {
    concurrency: 10,
    discard: true
  }));
  yield* Effect.addFinalizer(() => persistAssignments.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist assignments on shutdown", cause)), Effect.zipRight(persistRunners.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist runners on shutdown", cause))))));
  yield* Effect.forkIn(persistRunners, scope);
  // Rebalance immediately if there are unassigned shards
  yield* Effect.forkIn(rebalance(state.unassignedShards.length > 0), scope);
  // Start a regular cluster rebalance at the configured interval
  yield* rebalance(false).pipe(Effect.andThen(Effect.sleep(config.rebalanceInterval)), Effect.forever, Effect.forkIn(scope));
  yield* checkRunnerHealth.pipe(Effect.andThen(Effect.sleep(config.runnerHealthCheckInterval)), Effect.forever, Effect.forkIn(scope));
  yield* Effect.gen(function* () {
    const queue = yield* PubSub.subscribe(events);
    while (true) {
      yield* Effect.logInfo("Shard manager event:", yield* Queue.take(queue));
    }
  }).pipe(Effect.forkIn(scope));
  yield* Effect.logInfo("Shard manager initialized");
  return ShardManager.of({
    getAssignments,
    shardingEvents: PubSub.subscribe(events),
    register,
    unregister,
    rebalance,
    notifyUnhealthyRunner,
    checkRunnerHealth
  });
});
/**
 * @since 1.0.0
 * @category layer
 */
export const layer = /*#__PURE__*/Layer.scoped(ShardManager, make);
/**
 * @since 1.0.0
 * @category Server
 */
export const layerServerHandlers = /*#__PURE__*/Rpcs.toLayer(/*#__PURE__*/Effect.gen(function* () {
  const shardManager = yield* ShardManager;
  const clock = yield* Effect.clock;
  return {
    Register: ({
      runner
    }) => shardManager.register(runner),
    Unregister: ({
      address
    }) => shardManager.unregister(address),
    NotifyUnhealthyRunner: ({
      address
    }) => shardManager.notifyUnhealthyRunner(address),
    GetAssignments: () => shardManager.getAssignments,
    ShardingEvents: Effect.fnUntraced(function* () {
      const queue = yield* shardManager.shardingEvents;
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      yield* Queue.takeBetween(queue, 1, Number.MAX_SAFE_INTEGER).pipe(Effect.flatMap(events => mailbox.offerAll(events)), Effect.forever, Effect.forkScoped);
      return mailbox;
    }),
    GetTime: () => clock.currentTimeMillis
  };
}));
/**
 * @since 1.0.0
 * @category Server
 */
export const layerServer = /*#__PURE__*/RpcServer.layer(Rpcs, {
  spanPrefix: "ShardManager",
  disableTracing: true
}).pipe(/*#__PURE__*/Layer.provide(layerServerHandlers));
//# sourceMappingURL=ShardManager.js.map